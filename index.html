<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision - Zhou et al.">
  <meta name="description" content="StereoWalker augments navigation foundation models with stereo vision and mid-level vision priors for efficient dynamic urban navigation with only 1.5% training data.">
  <meta name="keywords" content="stereo vision, robot navigation, foundation models, mid-level vision, depth estimation, pixel tracking, urban navigation, machine learning, computer vision, AI">
  <!-- Authors -->
  <meta name="author" content="Wentao Zhou, Xuweiyi Chen, Vignesh Rajagopal, Jeffrey Chen, Rohan Chandra, Zezhou Cheng">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="University of Virginia">
  <meta property="og:title" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision">
  <meta property="og:description" content="StereoWalker augments navigation foundation models with stereo vision and mid-level vision priors for efficient dynamic urban navigation with only 1.5% training data.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Wentao Zhou">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- First author's Twitter handle -->
  <meta name="twitter:creator" content="@wentao_zhou">
  <meta name="twitter:title" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision">
  <meta name="twitter:description" content="StereoWalker augments navigation foundation models with stereo vision and mid-level vision priors for efficient dynamic urban navigation with only 1.5% training data.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision">
  <meta name="citation_author" content="Zhou, Wentao">
  <meta name="citation_author" content="Chen, Xuweiyi">
  <meta name="citation_author" content="Rajagopal, Vignesh">
  <meta name="citation_author" content="Chen, Jeffrey">
  <meta name="citation_author" content="Chandra, Rohan">
  <meta name="citation_author" content="Cheng, Zezhou">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- Paper title and authors -->
  <title>Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision - Zhou et al. | University of Virginia</title>
  
  <!-- Favicon and App Icons -->
  <!-- Using browser default favicon -->
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <!-- <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/figure-images.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "description": "StereoWalker augments navigation foundation models with stereo vision and mid-level vision priors for efficient dynamic urban navigation with only 1.5% training data.",
    "author": [
      {
        "@type": "Person",
        "name": "Wentao Zhou",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      },
      {
        "@type": "Person",
        "name": "Xuweiyi Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      },
      {
        "@type": "Person",
        "name": "Vignesh Rajagopal",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      },
      {
        "@type": "Person",
        "name": "Jeffrey Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      },
      {
        "@type": "Person",
        "name": "Rohan Chandra",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      },
      {
        "@type": "Person",
        "name": "Zezhou Cheng",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Virginia"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["stereo vision", "robot navigation", "foundation models", "mid-level vision", "depth estimation", "urban navigation", "machine learning", "computer vision"],
    "abstract": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient. We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "University of Virginia",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- Paper title -->
            <h1 class="title is-1 publication-title">Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors - First row -->
              <span class="author-block">
                <a href="#" target="_blank">Wentao Zhou</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Xuweiyi Chen</a>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Vignesh Rajagopal</a>
                  </span>
                  </div>
                  
                  <div class="is-size-5 publication-authors">
              <!-- Paper authors - Second row -->
                  <span class="author-block">
                    <a href="#" target="_blank">Jeffrey Chen</a>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Rohan Chandra</a>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Zezhou Cheng</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Virginia</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Paper button -->
                      <span class="link-block">
                        <a href="#" onclick="return false;" 
                        class="external-link button is-normal is-rounded is-dark" style="cursor: not-allowed; opacity: 0.7;">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (coming soon)</span>
                      </a>
                    </span>

                  <!-- Code button -->
                  <span class="link-block">
                    <a href="#" onclick="return false;"
                    class="external-link button is-normal is-rounded is-dark" style="cursor: not-allowed; opacity: 0.7;">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- arXiv button -->
                <span class="link-block">
                  <a href="#" onclick="return false;"
                  class="external-link button is-normal is-rounded is-dark" style="cursor: not-allowed; opacity: 0.7;">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (coming soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser figure -->
<section class="section" style="background-color: white; padding: 3rem 1.5rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem;">Overview</h2>
        <div class="figure-image-wrapper">
          <img src="static/images/teaser.jpg" alt="StereoWalker Overview" class="figure-image" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        </div>
        <div class="content has-text-justified" style="margin-top: 2.5rem; padding: 1.5rem; background-color: #f8fafc; border-radius: 8px; border-left: 4px solid #2563eb;">
          <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin: 0;">
            <strong style="color: #2563eb; font-size: 1.1rem;">(a)</strong> Existing visual navigation foundation models (NFMs) process monocular inputs and predict actions without any intermediate vision modules. 
            <strong style="color: #2563eb; font-size: 1.1rem;">(b)</strong> StereoWalker improves this paradigm by incorporating stereo and mid-level vision modules such as depth estimation and dense point tracking. 
            <strong style="color: #2563eb; font-size: 1.1rem;">(c)</strong> Relative to CityWalker, the current state of the art, StereoWalker reaches high training efficiency and improved navigation accuracy while using only <span style="color: #dc2626; font-weight: 600;">1.5%</span> of the training data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient. We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Main figure -->
<section class="section figure-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <div class="figure-image-wrapper" style="max-width: 100%; margin: 0 auto; overflow: visible;">
          <img src="static/images/main.jpg" alt="StereoWalker Method" class="figure-image" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); transform: scale(1.3); margin: 3rem 0;">
        </div>
        <div class="content has-text-justified" style="margin-top: 4.5rem; padding: 1.5rem; background-color: #f0f9ff; border-radius: 8px; border-left: 4px solid #0ea5e9;">
          <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-bottom: 1.2rem;">
            While prior visual navigation models compress each frame to a single DINOv2 <code style="background-color: #e2e8f0; padding: 2px 6px; border-radius: 4px; color: #1e293b; font-size: 0.95em;">[CLS]</code> token, StereoWalker retains <em style="color: #0284c7; font-weight: 500;">all</em> patch tokens to preserve fine-grained spatial structure critical for control. Our intuition is straightforward: accurate navigation demands richer visual perception than a global summary can provide.
          </p>
          <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin: 0;">
            As shown in the figure above, given a short temporal window of rectified stereo (or monocular) frames 
            <span style="font-family: 'Courier New', monospace; color: #0f172a; font-weight: 500;">ùí±<sub>t-N+1:t</sub></span>
            and their corresponding positions 
            <span style="font-family: 'Courier New', monospace; color: #0f172a; font-weight: 500;">ùí´<sub>t-N+1:t</sub></span>,
            the model forms dense mid-level tokens that jointly encode appearance, geometry, and short-term motion cues. Tokens from all frames are then processed by three stages:
          </p>
          <div style="margin-top: 1rem; margin-left: 1.5rem;">
            <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin: 0.5rem 0;">
              <strong style="color: #0ea5e9;">(i)</strong> tracking-guided attention to maintain temporal correspondence and reduce drift,
            </p>
            <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin: 0.5rem 0;">
              <strong style="color: #0ea5e9;">(ii)</strong> global attention to integrate scene context across views, and
            </p>
            <p style="font-size: 1.05rem; line-height: 1.8; color: #334155; margin: 0.5rem 0;">
              <strong style="color: #0ea5e9;">(iii)</strong> target-token attention to focus prediction on goal-relevant regions.
            </p>
          </div>
          <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-top: 1rem; margin-bottom: 0;">
            StereoWalker supports both stereo and monocular inputs with the same architecture, differing only in tokenization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End main figure -->

<!-- MAOE figure -->
<section class="section figure-section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Results</h2>
        <div class="figure-image-wrapper">
          <img src="static/images/maoe.jpg" alt="StereoWalker Results" class="figure-image" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        </div>
        
        <!-- Analysis of Mid-level Vision -->
        <div class="content has-text-justified" style="margin-top: 3rem;">
          <h3 style="font-weight: 700; color: #1e293b; margin-bottom: 1.5rem;">Analysis of Mid-level Vision</h3>
          <div style="padding: 1.5rem; background-color: white; border-radius: 8px; border-left: 4px solid #10b981;">
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-bottom: 1.2rem;">
              Our ablation analysis evaluates different architectural configurations on the CityWalker teleoperation benchmark. All variants are trained on the same monocular dataset, with specific components selectively enabled or disabled for a fair comparison. Earlier baselines such as ViNT, GNM, NoMaD, and CityWalker represent each image using only a single <code style="background-color: #e2e8f0; padding: 2px 6px; border-radius: 4px; color: #1e293b; font-size: 0.95em;">[CLS]</code> token. In contrast, we observe that using all patch tokens to capture finer spatial information leads to an immediate and significant improvement in the Mean Angular Orientation Error (MAOE). 
            </p>
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-bottom: 1.2rem;">
              Building upon this representation, we observe that incorporating depth and dense pixel tracking further enhances navigation accuracy, as these two mid-level cues provide complementary inductive signals. Depth captures the three-dimensional structure, yielding a substantial reduction in MAOE relative to the patch token model. Tracking encodes scene motion and temporal consistency, and incorporating tracking on top of patch tokens and depth provides additional performance gains.
            </p>
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin: 0;">
              Prior studies demonstrated similar advantages of mid-level vision in controlled or static environments. Our experiments in large-scale dynamic urban navigation show that explicitly modeling depth and motion significantly improves robustness and effectiveness in real-world conditions. Fine-tuned StereoWalker shows substantial improvements across Forward, Left turn, and Right turn scenarios.
            </p>
          </div>
        </div>
        
        <!-- Analysis of Training Efficiency -->
        <div class="content has-text-justified" style="margin-top: 2rem;">
          <h3 style="font-weight: 700; color: #1e293b; margin-bottom: 1.5rem;">Analysis of Training Efficiency</h3>
          <div style="padding: 1.5rem; background-color: white; border-radius: 8px; border-left: 4px solid #8b5cf6;">
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-bottom: 1.2rem;">
              Beyond performance gains, we observe that incorporating mid-level vision capabilities significantly accelerates training. We carefully train our model on monocular data using only a fraction of the original dataset, achieving comparable performance with merely <span style="color: #dc2626; font-weight: 600;">1.5%</span> of CityWalker's training data. Under different amounts of training data, we use the same training settings for both CityWalker and our model.
            </p>
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin-bottom: 1.2rem;">
              Enabling patch tokens introduces richer visual representations but also necessitates architectural modifications to the decoder, as our model no longer relies on a single <code style="background-color: #e2e8f0; padding: 2px 6px; border-radius: 4px; color: #1e293b; font-size: 0.95em;">[CLS]</code> token representation. Consequently, our model with patch tokens alone requires additional training time to match CityWalker's performance. However, once depth cues are injected, the model quickly outperforms CityWalker.
            </p>
            <p style="font-size: 1.05rem; line-height: 1.9; color: #334155; margin: 0;">
              Further incorporating both depth and tracking information leads to faster convergence and superior performance, surpassing CityWalker trained with over 2,000 hours of monocular videos. This demonstrates that mid-level vision not only enhances representation quality but also provides strong inductive biases that make training more data- and time-efficient.
            </p>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>
<!-- End MAOE figure -->

<!-- Visualization figure -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Visualization</h2>
        <div class="figure-image-wrapper">
          <img src="static/images/visualization.jpg" alt="StereoWalker Visualization Results" class="figure-image" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End visualization figure -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>Coming soon...</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
